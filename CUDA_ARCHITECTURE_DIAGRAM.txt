═══════════════════════════════════════════════════════════════════════════════
                    CUDA ENERGY STORMS - ARCHITECTURE DIAGRAM
═══════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────┐
│                          EXECUTION FLOW                                     │
└─────────────────────────────────────────────────────────────────────────────┘

    HOST (CPU)                                    DEVICE (GPU)
    ═════════                                     ════════════

1. INITIALIZATION
   ┌──────────────┐
   │ Load storms  │
   │ Compute sqrt │
   │ table (host) │
   └──────┬───────┘
          │
          │ cudaMalloc()
          ├─────────────────────────────────→ ┌──────────────────┐
          │                                    │ Allocate d_layer │
          │                                    │ Allocate d_sqrt  │
          │                                    │ Allocate buffers │
          │                                    └──────────────────┘
          │
          │ cudaMemcpy(H→D)
          ├─────────────────────────────────→ ┌──────────────────┐
          │                                    │ Copy sqrt_table  │
          │                                    │ Initialize layer │
          │                                    └──────────────────┘


2. STORM LOOP (for each storm)
   
   FOR EACH PARTICLE:
   ┌──────────────┐
   │ Get position │
   │ Get energy   │
   └──────┬───────┘
          │
          │ Kernel Launch <<<blocks, threads>>>
          │
          ├─────────────────────────────────→ ┌────────────────────────────┐
          │                                    │  KERNEL 1: UPDATE ENERGY   │
          │                                    ├────────────────────────────┤
          │                                    │                            │
          │                                    │  Grid: 4 blocks            │
          │                                    │  ┌─────┬─────┬─────┬─────┐│
          │                                    │  │ B0  │ B1  │ B2  │ B3  ││
          │                                    │  └─────┴─────┴─────┴─────┘│
          │                                    │                            │
          │                                    │  Each Block: 256 threads   │
          │                                    │  ┌─┬─┬─┬─┬─┬─┬─┬─┬ ... ┐  │
          │                                    │  │T│T│T│T│T│T│T│T│ ... │  │
          │                                    │  │0│1│2│3│4│5│6│7│     │  │
          │                                    │  └─┴─┴─┴─┴─┴─┴─┴─┴ ... ┘  │
          │                                    │                            │
          │                                    │  Each thread:              │
          │                                    │  1. Compute distance       │
          │                                    │  2. Lookup sqrt_table      │
          │                                    │  3. Calculate energy       │
          │                                    │  4. atomicAdd to layer[k]  │
          │                                    │                            │
          │                                    └────────────────────────────┘
          │
   (Repeat for all particles in storm)
          │
          │ cudaDeviceSynchronize()
          ├─────────────────────────────────→ [Wait for all kernels]
          │

   RELAXATION:
   ┌──────────────┐
   │ Prepare      │
   └──────┬───────┘
          │
          │ cudaMemcpy(D→D)
          ├─────────────────────────────────→ ┌──────────────────┐
          │                                    │ Copy layer to    │
          │                                    │ layer_copy       │
          │                                    └──────────────────┘
          │
          │ Kernel Launch
          ├─────────────────────────────────→ ┌────────────────────────────┐
          │                                    │  KERNEL 2: RELAXATION      │
          │                                    ├────────────────────────────┤
          │                                    │                            │
          │                                    │  Each thread computes:     │
          │                                    │                            │
          │                                    │  layer[k] =                │
          │                                    │    (layer_copy[k-1] +      │
          │                                    │     layer_copy[k] +        │
          │                                    │     layer_copy[k+1]) / 3   │
          │                                    │                            │
          │                                    │  Visualization:            │
          │                                    │  ┌──┬──┬──┬──┬──┬──┐      │
          │                                    │  │10│20│50│30│15│ 5│ old  │
          │                                    │  └──┴──┴──┴──┴──┴──┘      │
          │                                    │     ↓  ↓  ↓  ↓  ↓  ↓       │
          │                                    │  ┌──┬──┬──┬──┬──┬──┐      │
          │                                    │  │10│27│33│32│17│ 5│ new  │
          │                                    │  └──┴──┴──┴──┴──┴──┘      │
          │                                    │                            │
          │                                    └────────────────────────────┘

   FIND MAXIMUM:
          │
          │ Kernel Launch
          ├─────────────────────────────────→ ┌────────────────────────────┐
          │                                    │  KERNEL 3: FIND MAXIMA     │
          │                                    ├────────────────────────────┤
          │                                    │                            │
          │                                    │  Each thread checks:       │
          │                                    │                            │
          │                                    │  if (layer[k] > layer[k-1] │
          │                                    │    && layer[k] > layer[k+1])│
          │                                    │  {                         │
          │                                    │    max_values[k] = layer[k]│
          │                                    │    max_positions[k] = k    │
          │                                    │  } else {                  │
          │                                    │    max_values[k] = -1      │
          │                                    │  }                         │
          │                                    │                            │
          │                                    └────────────────────────────┘
          │
          │ cudaMemcpy(D→H)
   ┌──────┴───────┐ ←────────────────────── Copy candidates only
   │ Receive      │
   │ candidates   │
   └──────┬───────┘
          │
   ┌──────┴───────┐
   │ Find global  │
   │ maximum (CPU)│
   │ Simple loop  │
   └──────────────┘

(Repeat for next storm)


3. CLEANUP
   ┌──────────────┐
   │ Return       │
   │ results      │
   └──────┬───────┘
          │
          │ cudaFree()
          ├─────────────────────────────────→ ┌──────────────────┐
          │                                    │ Free all GPU     │
          │                                    │ memory           │
          │                                    └──────────────────┘
          │
   ┌──────┴───────┐
   │ free()       │
   │ CPU memory   │
   └──────────────┘


═══════════════════════════════════════════════════════════════════════════════
                            MEMORY LAYOUT
═══════════════════════════════════════════════════════════════════════════════

HOST MEMORY                              DEVICE MEMORY
──────────────────                       ────────────────────────────────────

┌──────────────────┐                     ┌──────────────────────────────────┐
│ h_sqrt_table     │──────copy once────→ │ d_sqrt_table                     │
│ [1.0, 1.41, ...] │                     │ [1.0, 1.41, 1.73, 2.0, ...]      │
└──────────────────┘                     │ Size: ~max_distance floats       │
                                         └──────────────────────────────────┘

┌──────────────────┐                     ┌──────────────────────────────────┐
│ storms[]         │──────params only──→ │ Used by kernels                  │
│ - positions      │                     │ (no array copy needed)           │
│ - energies       │                     │                                  │
└──────────────────┘                     └──────────────────────────────────┘

                                         ┌──────────────────────────────────┐
                                         │ d_layer                          │
                                         │ [0.0, 0.0, 0.0, 0.0, ...]        │
                    ┌────updated─────────│ Size: layer_size floats          │
                    │  continuously      │ STAYS ON GPU!                    │
                    │                    └──────────────────────────────────┘
                    │
                    │                    ┌──────────────────────────────────┐
                    │                    │ d_layer_copy                     │
                    └────copied for──────│ [temp copy for relaxation]       │
                      relaxation         │ Size: layer_size floats          │
                                         └──────────────────────────────────┘

┌──────────────────┐                     ┌──────────────────────────────────┐
│ h_max_values[]   │←───copy results───  │ d_max_values[]                   │
│ [-1, 50.3, -1, .]│    per storm        │ [-1.0, 50.3, -1.0, 100.2, ...]   │
└──────────────────┘                     │ Size: num_candidates floats      │
                                         └──────────────────────────────────┘
┌──────────────────┐                     ┌──────────────────────────────────┐
│ h_max_positions[]│←───copy results───  │ d_max_positions[]                │
│ [-1, 523, -1, .] │    per storm        │ [-1, 523, -1, 1042, ...]         │
└──────────────────┘                     │ Size: num_candidates ints        │
                                         └──────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════
                        THREAD MAPPING EXAMPLE
═══════════════════════════════════════════════════════════════════════════════

Layer Size: 1000
Block Size: 256
Number of Blocks: (1000 + 256 - 1) / 256 = 4

┌─────────────────────────────────────────────────────────────────────────────┐
│                              GRID                                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  BLOCK 0                BLOCK 1                BLOCK 2         BLOCK 3      │
│  ┌────────────┐        ┌────────────┐        ┌────────────┐  ┌──────────┐ │
│  │Thread 0    │        │Thread 0    │        │Thread 0    │  │Thread 0  │ │
│  │Thread 1    │        │Thread 1    │        │Thread 1    │  │Thread 1  │ │
│  │Thread 2    │        │Thread 2    │        │Thread 2    │  │Thread 2  │ │
│  │   ...      │        │   ...      │        │   ...      │  │  ...     │ │
│  │Thread 255  │        │Thread 255  │        │Thread 255  │  │Thread 231│ │
│  └────────────┘        └────────────┘        └────────────┘  └──────────┘ │
│       ↓                     ↓                     ↓                ↓        │
│  Cells 0-255           Cells 256-511         Cells 512-767   Cells 768-999 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

Thread ID Calculation:
  Block 0, Thread 0:   k = 0 * 256 + 0   = 0     → layer[0]
  Block 0, Thread 1:   k = 0 * 256 + 1   = 1     → layer[1]
  Block 0, Thread 255: k = 0 * 256 + 255 = 255   → layer[255]
  Block 1, Thread 0:   k = 1 * 256 + 0   = 256   → layer[256]
  Block 1, Thread 255: k = 1 * 256 + 255 = 511   → layer[511]
  Block 3, Thread 231: k = 3 * 256 + 231 = 999   → layer[999]
  Block 3, Thread 232: k = 3 * 256 + 232 = 1000  → OUT OF BOUNDS! (caught by if)


═══════════════════════════════════════════════════════════════════════════════
                       PERFORMANCE COMPARISON
═══════════════════════════════════════════════════════════════════════════════

SEQUENTIAL CPU                          CUDA GPU
─────────────────────────────────────   ──────────────────────────────────────

For 1 particle, layer size 1M:         For 1 particle, layer size 1M:

for (k = 0; k < 1M; k++) {             Launch kernel with 1M threads
  distance = abs(pos - k) + 1;         ALL run simultaneously:
  atten = sqrt(distance);  ← SLOW!       distance = abs(pos - k) + 1;
  energy = base / atten;                 atten = sqrt_table[distance];
  if (energy > thresh)                   energy = base / atten;
    layer[k] += energy;                  if (energy > thresh)
}                                          atomicAdd(&layer[k], energy);

Time: ~10ms per particle               Time: ~0.1ms per particle
                                       
For 10 storms × 1000 particles:        For 10 storms × 1000 particles:
Total: ~100 seconds                    Total: ~1 second

                                       SPEEDUP: ~100x


═══════════════════════════════════════════════════════════════════════════════
                            KEY CONCEPTS
═══════════════════════════════════════════════════════════════════════════════

1. PARALLELISM
   ────────────
   CPU: 1 core processes cells one-by-one     [→→→→→→→→→→]
   GPU: 1000s of cores process simultaneously [↓↓↓↓↓↓↓↓↓↓]

2. MEMORY HIERARCHY
   ────────────────
   Fast but small:  GPU registers > GPU shared memory > GPU global memory
   Slow but large:  GPU global memory > CPU-GPU transfer > CPU RAM

3. ATOMIC OPERATIONS
   ─────────────────
   Problem: Multiple threads updating same cell simultaneously
   Solution: atomicAdd() ensures one-at-a-time access (hardware supported)

4. COALESCED ACCESS
   ────────────────
   Good: Threads 0-31 access layer[0-31]   [consecutive = FAST]
   Bad:  Threads 0-31 access random cells  [scattered = SLOW]

5. KERNEL LAUNCH
   ─────────────
   kernel<<<numBlocks, threadsPerBlock>>>(args)
          │           │
          │           └─ How many threads per block (e.g., 256)
          └─ How many blocks (e.g., 4)
   
   Total threads = numBlocks × threadsPerBlock

═══════════════════════════════════════════════════════════════════════════════
